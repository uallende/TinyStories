{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.tokenizer import Tokenizer as T\n",
    "import torch\n",
    "from Classes.myGPT import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_text = \"Once upon a time\"\n",
    "num_return_sequences = 4\n",
    "tkzr = T()\n",
    "init_toks = torch.tensor(tkzr.encode(initial_text, eos=True, bos=False), dtype=torch.long)\n",
    "init_toks = init_toks.unsqueeze(0).repeat(num_return_sequences, 1).to('cuda')\n",
    "\n",
    "block_size = 256 \n",
    "batch_size = 32\n",
    "run_count = 0\n",
    "batch_size_values = [40]\n",
    "n_heads = 6\n",
    "n_layers = 6\n",
    "d_model = 768\n",
    "dropout = 0.1\n",
    "eval_iters = 10\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 150\n",
    "max_steps = 57876 # 57876 steps is 1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
    "vocab_size = 15_000 # next power of two doesn't increase performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4036, -0.1967, -0.6080,  ...,  0.3179, -0.2770, -0.2728],\n",
       "          [ 0.4981,  0.7601,  0.8514,  ..., -1.3448, -0.7438, -1.3059],\n",
       "          [-0.3689, -0.1121, -0.5529,  ..., -1.1671, -0.4127,  0.4858],\n",
       "          [ 0.4084,  0.6332, -0.5048,  ..., -2.0224,  0.1243, -0.6245],\n",
       "          [-0.3726,  1.1812,  0.0185,  ...,  0.6581,  0.7824,  0.0134]],\n",
       " \n",
       "         [[ 0.2733, -0.0587, -0.4586,  ...,  0.3827, -0.1288, -0.1896],\n",
       "          [ 0.3984,  0.9730,  0.7608,  ..., -1.1682, -0.6709, -1.2315],\n",
       "          [-0.2386, -0.3481, -0.7343,  ..., -1.1977, -0.3440,  0.4849],\n",
       "          [ 0.3570,  0.5625, -1.0855,  ..., -2.1406,  0.0465, -0.3471],\n",
       "          [-0.3281,  1.1905, -0.2178,  ...,  0.4984,  0.8839, -0.1948]],\n",
       " \n",
       "         [[-0.1265,  0.2764, -0.2417,  ...,  0.2611, -0.2767, -0.3928],\n",
       "          [ 0.4434,  0.6976,  0.4759,  ..., -0.9876, -0.6350, -1.4394],\n",
       "          [-0.1974, -0.2689, -0.2861,  ..., -1.3900, -0.6359,  0.4553],\n",
       "          [ 0.2432,  0.6340, -0.6824,  ..., -2.1271,  0.0590, -0.8043],\n",
       "          [-0.3732,  1.2068, -0.2606,  ...,  0.7187,  0.7878,  0.0443]],\n",
       " \n",
       "         [[ 0.4163, -0.1358, -0.2855,  ...,  0.4921, -0.5093, -0.1952],\n",
       "          [ 0.3076,  1.0595,  0.9164,  ..., -1.1532, -0.5712, -1.1209],\n",
       "          [-0.0246, -0.2883, -0.6157,  ..., -1.2113, -0.6325,  0.5345],\n",
       "          [ 0.1549,  0.5975, -0.5949,  ..., -2.1138, -0.0231, -0.4291],\n",
       "          [-0.1646,  1.0548, -0.1299,  ...,  0.7128,  1.1071, -0.2365]]],\n",
       "        device='cuda:0', grad_fn=<UnsafeViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(block_size=block_size, dff = d_model*3,\n",
    "              n_heads=n_heads, n_layers=n_layers, d_model=d_model, \n",
    "              dropout=dropout,  vocab_size=vocab_size).to('cuda')\n",
    "\n",
    "model(init_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
